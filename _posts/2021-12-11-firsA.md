---
layout: single
title:  "Chapter2. Perceptron"
categories: ['Deep Learning from Scratch']
tag: ['deep learning', 'perceptron']
---

# Chapter2. Percetron
In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers. 
A binary classifier is a function which can decide whether or not an input, 
represented by a vector of numbers, belongs to some specific class

Equation for Perceptron:  
y = 0 (w1x1 + w2x2 <= θ)  
y = 1 (w1x2 + w2x2 >  θ)  

As we see in the equation, we add weights on each imput(x1,x2)
which implies that as bigger the weight is, it gets more significant.

## Appying Perceptron on "AND Gate"
AND gate has two inputs and one output.
The output will be '1' only when it has '1' for both inputs.
Drawing a truth table for AND gate,  
x1  x2  y  
0   0   0  
1   0   0  
0   1   0  
1   1   1  

How do we express the AND gate by using perceptron?
-> We need to define (w1, w2, θ) that would work as the truth table above.  
ex. (0.5, 0.5 0.7), (0.5, 0.5,0.8), ...

## Applying Perceptron on "NAND Gate" and "OR Gate"
NAND means "Not AND".
It's bascially reversed version of And gate.

We can simply apply NAND gate on perceptron with combinations like  
(w1, w2, θ) = (-.5, -.5 -.7)


OR Gate would print '1' when one of the input is bigger than 1.  
(w1, w2, θ) = (0.5, 0.5, -0.2)


## implementing Percetron on Python
```python
## AND Gate
```


```python
def AND(x1, x2):
    w1, w2 , theta = 0.5, 0.5, 0.7
    tmp = x1*w1 + x2*w2
    if tmp <= theta:
        return 0
    elif tmp > theta:
        return 1
```


```python
[AND(0,0),AND(1,0),AND(0,1),AND(1,1)]
```




    [0, 0, 0, 1]




```python
## adding weights
import numpy as np
x = np.array([0,1])
w = np.array([.5,.5])
b = -.7
```


```python
w*x
```




    array([0. , 0.5])




```python
np.sum(w*x)
```




    0.5




```python
np.sum(w*x) +b
```




    -0.19999999999999996




```python
def AND(x1, x2):
    x = np.array([x1,x2])
    w = np.array([.5,.5])
    b = -.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0 
    else:
        return 1
```


```python
[AND(0,1), AND(1,.5)]
```




    [0, 1]




```python
## NAND Gate
```


```python
def NAND(x1, x2):
    x = np.array([x1,x2])
    w= np.array([-.5, -.5])
    b = .7
    tmp = np.sum(w*x)+b
    if tmp <=0:
        return 0
    else:
        return 1
```


```python
[NAND(0,0),NAND(1,0),NAND(0,1),NAND(1,1)]
```




    [1, 1, 1, 0]




```python
## OR Gate
```


```python
def OR(x1,x2):
    x = np.array([x1,x2])
    w = np.array([.5,.5])
    b = -.2
    tmp = np.sum(w*x)+b
    if tmp <= 0:
        return 0 
    else:
        return 1
```


```python
[OR(1,0),OR(0,1),OR(0,0),OR(1,1)]
```




    [1, 1, 0, 1]

## Limtation of Perceptron
As we've seen above, we can implement AND, NAND, OR gate by using perceptron.
But how about XOR gate?  
XOR gates only produce '1' when both inputs doesn't match and a perceptron can only converge on linearly separable data.   
Therefore, it isn't capable of imitating the XOR function


## Multi-Layer Perceptron
To deal with the limitation of perceptron, we can use multilayer perceptron.
we make combinations of AND, NAND, OR gates to implement XOR gate.  

## Implementing XOR Gate on Python




```python
## XOR Gate
```


```python
def XOR(x1,x2):
    s1 = NAND(x1,x2)
    s2 = OR(x1,x2)
    y = AND(s1,s2)
    return y
```


```python
[XOR(0,0),XOR(0,1),XOR(1,0),XOR(1,1)]
```




    [0, 1, 1, 0]


Reference: Weidman, S. (2019). Deep learning from scratch: Building with python from first principles. O’Reilly.  
https://github.com/Jangjh93/deep-learning-from-scratch/tree/master/ch02
 

