---
layout: single
title:  "Chapter3. Neural Network"
categories: ['Deep Learning from Scratch']
tag: ['deeplearning', 'neural network']
---

As we've learned in Chapter2, in theory, we can implement complicated logics with perceptron.
However, it still requires human to do the calculations to select proper values for weights.  
Neural Network can solve this problem.

## Perceptron to Neural Network
A neural network is a network or circuit of neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or node.  

<img width="626" alt="image1" src="https://user-images.githubusercontent.com/69702946/145707815-3975dc90-be91-46c3-832f-5a2da5b77832.png">


Let's review perceptron first.  
y = 0(b+w1x1 + w2x2 <=0) where b is bias  
y = 1(b+w1x1 + w2x2 > 0) where b is bias   
  
Making the eqautions simpler,  
y = h(b + w1x1 + w2x2)  
h(x) = 0 (x <= 0)  
h(x) = 1 (x >  0)  
h(x) is called Activation Fucntion.  


## Activation function
An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network.    
![image2](https://user-images.githubusercontent.com/69702946/145707825-164fa769-e9dc-480b-9043-95c81c855581.png)


In equation,  
a = b+ w1x1 + w2x2  
y = h(a)  
  
### Sigmoid function
A sigmoid function is a bounded, differentiable, real function that is defined for all real input values and has a non-negative derivative at each point
and exactly one inflection point. A sigmoid "function" and a sigmoid "curve" refer to the same object.
h(x) = 1/(1+exp(-x))  

### Drawing Sigmoid Function on Python


```python
import numpy as np
import matplotlib.pyplot as plt
```


```python
def sigmoid(x):
    return 1/(1+np.exp(-x))
```


```python
x = np.array([-1.0, 1.0, 2.0])
print(sigmoid(x))
```

    [0.26894142 0.73105858 0.88079708]
    


```python
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x,y)
plt.ylim(-0.1, 1.1)
plt.show()
```
![output_23_0](https://user-images.githubusercontent.com/69702946/145707768-c05eb1cf-f74a-4edf-bb3a-1b454944bc2a.png)


### Step Function
Binary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold neuron is deactivated.


### Drawing Step Function on Python

```python
def step_function(x):
    return np.array(x>0, dtype=np.int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x,y)
plt.ylim(-0.1,1.1)
plt.show()
```

![output_24_0](https://user-images.githubusercontent.com/69702946/145708297-d14c2bd7-f736-448e-a807-6c3922cb484b.png)



## Sigmoid Function VS Step Function
They both produce 1 as the input values getting bigger and 0 vice versa.
Also, they're non-linear functions. 
But as we clearly see in the graphs, the biggest differencee between them would be the "flexibility".


## Nonlinear Function
Activation functions cannot be linear because neural networks with a linear activation function are effective only one layer deep, 
regardless of how complex their architecture is. 
Input to networks is usually linear transformation (input * weight), but real world and problems are non-linear.  
In short, there's no point to use a linear activation function for neural networks.  
Mathmatically speaking, think of using three layer activation function that uses a linear function  
h(x) = cx  
In equation, y(x) = h(h(h(x))) = c*c*c*x.
 
This is basically the same as y(x) = ax where a = c^3.

### ReLU Function
In the context of artificial neural networks, the rectifier or ReLU activation function is an activation function defined as the positive part of its argument:
h(x) = x (x >  0)  
h(x) = 0 (x <= 0)  

### Implementing ReLU Funion on Python
```python
import numpy as np
import matplotlib.pyplot as plt
```


```python
def relu(x):
    return np.maximum(0,x)
```


```python
x = np.arange(-2,3)
y = relu(x)
plt.plot(x,y)
plt.ylim(-0.5, 2)
plt.xlim(-1,2)
plt.title("ReLU Function")
plt.show()
```


![output_2_0](https://user-images.githubusercontent.com/69702946/145712805-695a7fb9-6339-4b3d-9485-ddfb394aae52.png)

## Calculation of Multidimensional Array
We can effectively implement neural network by calculation of multimensional array.  
```python
## One-Dimensional Array
```


```python
A = np.array([1,2,3,4])
print(A)
```

    [1 2 3 4]
    


```python
np.ndim(A)
```




    1




```python
A.shape
```




    (4,)




```python
## Two-Dimensional Array
```


```python
A = np.array([[1,2], [3,4]])
B = np.array([[5,6], [7,8]])
```


```python
[A.shape, B.shape]
```




    [(2, 2), (2, 2)]




```python
np.dot(A,B)
```




    array([[19, 22],
           [43, 50]])





## Implementing Three layer Neural Network



![image3](https://user-images.githubusercontent.com/69702946/145713048-e41a1dfa-df9a-46e6-b8d3-f0ee14a17c3e.png)

```python
# Input layer to the first layer
# (Using sigmoid function as an activation function)
```


```python
X = np.array([1.0, 0.5])
W1 = np.array([[0.1,0.3,0.5], [0.2,0.4,0.6]])
B1 = np.array([0.1,0.2,0.3])
print("W1 dim.:", W1.shape)
print("X dim.:", X.shape)
print("B1 dim.:",B1.shape)
```

    W1 dim.: (2, 3)
    X dim.: (2,)
    B1 dim.: (3,)
    


```python
A1  = np.dot(X,W1) + B1
```


```python
def sigmoid(x):
    return 1/(1+np.exp(-x))
```


```python
Z1 = sigmoid(A1)
print(Z1)
```

    [0.57444252 0.66818777 0.75026011]
    


```python
# First layer to the second layer
# (Using sigmoid function as an activation function)
```


```python
W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
B2 = np.array([0.1,0.2])

print("Z1 dim.:", Z1.shape)
print("W2 dim.:", W2.shape)
print("B2 dim.:", B2.shape)
```

    Z1 dim.: (3,)
    W2 dim.: (3, 2)
    B2 dim.: (2,)
    


```python
A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)
```


```python
# Second layer to the output layer
```


```python
def identify_function(x):
    return x

W3 = np.array([[0.1,0.3],[0.2,0.4]])
B3 = np.array([0.1,0.2])

A3 = np.dot(Z2,W3) + B3
Y = identify_function(A3)
print(Y)
```

    [0.31682708 0.69627909]
    

## Identify Function and Softmax Function
Ienfify function produce the input values.  
Softmax function will always produce values that range 0 to 1. Because of this, we can interpret the output as "probability". 
The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. 
That is, softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels.  
In equation, yk = exp(ak)/nâˆ‘i=1 exp(ai)  

```python
# Implementing softmax function
```


```python
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a/sum_exp_a
    return y
```


```python
a = np.array([0.3,2.9,4.0])
print(softmax(a))
```

    [0.01821127 0.24519181 0.73659691]
    
    
### Caution When Implementing Softmax Function
When you implement softmax function on computer, it could cause an overflow problem.  
Since softmax function uses exponential function, it will produce big numbers.
For example, exp(1000) will product -inf and when you do the division with infinite values, 
the result will be most likely unstable.  
To solve this problem, we can normalize the softmax function.


![image4](https://user-images.githubusercontent.com/69702946/145715048-3a44072f-403e-4c43-8ae6-d8e787e42cd5.png)

```python
import warnings
```


```python
# When softmax function is not normalized
# it produces nans
warnings.filterwarnings("ignore")
a = np.array([1010,1000,990])
np.exp(a)/np.sum(np.exp(a))
```




    array([nan, nan, nan])




```python
# When softmax function is normalized
def adj_softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a/sum_exp_a
    
    return y
```


```python
adj_softmax(a)
```




    array([9.99954600e-01, 4.53978686e-05, 2.06106005e-09])


### Selecting the Number of Neuron.
It's common to select the same number as the number of class in a classification problem.
For example, if you want to classify an image of numbers 0-9, you would select 10 neurons.


## Classifying Handwritten Numbers
We will cover foward propagation here. Forward Propagation is the way to move from the Input layer (left) to the Output layer (right) in the neural network. 


