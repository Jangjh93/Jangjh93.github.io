---
layout: single
title:  "Chapter4. Neural Network Training"
categories: ['Deep Learning from Scratch']
tag: ['deep learning', 'neural network']
---

Neural network training is a process that will automatically produce the optimized weight values.
A loss function is function that shows error rate of prediction. An optimization problem seeks to minimize a loss function.
We would need to find weight and bias to minimize a loss function.


### Sum of Squares for Error(SSE)
E = 1/2 ∑(yk-tk)^2 where   
yk stands for the output value  
tk stands for the label  
k stands for the dimension  
 
```python
import numpy as np
```


```python
# Define Sum of Squres for Error(SSE)
def sum_squares_error (y,t):
    return 0.5 * np.sum((y-t)**2)
```


```python
# When the output predicted the probability
# of the correct answer '2' is the higest(0.6)
# -> predicted correctly
y = [.1, .05, .6, 0, .05, .1, 0, .1, 0, 0] # output values
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # label(the answer is actually '2')
```


```python
sum_squares_error(np.array(y), np.array(t))
```




    0.09750000000000003




```python
# When the output predicted the probability
# of the correct answer '7' is the higest(0.6)
# -> predicted incorrectly
y = [.1, .05, 0.1, 0, .05, .1, 0, .6, 0, 0] # output values
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # label(the answer is actually '2')
```


```python
sum_squares_error(np.array(y), np.array(t))
```




    0.5975

### Cross Entropy Error(CEE)
E = -∑tk * logyk where  
yk is the output  
tk is the label(only the index for the correct answer is 1 and 0 for else - One Hot Encoding).  

```python
# Define Cross Entropy Error(CEE)
def cross_entropy_error(y,t):
    delta = 1e-7 #preventing an overflow
    return -np.sum(t * np.log(y+delta))
```


```python
# When the output predicted the probability
# of the correct answer '2' is the higest(0.6)
# -> predicted correctly
y = [.1, .05, .6, 0, .05, .1, 0, .1, 0, 0] # output values
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # label(the answer is actually '2')
```


```python
cross_entropy_error(np.array(y),np.array(t))
```




    0.510825457099338




```python
# When the output predicted the probability
# of the correct answer '7' is the higest(0.6)
# -> predicted incorrectly
y = [.1, .05, 0.1, 0, .05, .1, 0, .6, 0, 0] # output values
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # label(the answer is actually '2')
```


```python
cross_entropy_error(np.array(y),np.array(t))
```




    2.302584092994546


### Mini Batch
When we have multiple training dataset, we need to use sum of loss functions of each training set.
In equation,  
E = -1/N ∑ ∑tnk logyk  

However, as the size of dataset gets bigger, computing gets costly. It's not realistict to compute loss function in big data.
This is why we use mini batch which randomly select a small number of data and train them.
