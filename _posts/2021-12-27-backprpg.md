---
layout: single
title:  "Chapter5. Backpropagation"
categories: ['Deep Learning from Scratch']
tag: ['deep learning', 'Backpropagation']
---

Numerical differentication is a simple way to calculate gradient descent(gradient descent of loss function for the parameters). 
However, it takes too long to compute.  
Instead, we can use back propagation which can compute gradient descent more efficiently. 
There are two ways to understand backpropagation:  
1. By the equation(a general way in machine learning)  
2. By the computational graph  

## Computational graph
We visualize the process of computing with node and edge(the line between nodes).
Forward propagation is the way to move from the input layer (left) to the output layer (right) in the neural network. 
The process of moving from the right to left i.e backward from the output to the input layer is called the backward propagation. 
backward propagation is important when calculating differentiation.  

### Advantage of computational graph
...
