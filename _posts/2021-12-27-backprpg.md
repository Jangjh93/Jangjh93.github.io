---
layout: single
title:  "Chapter5. Backpropagation"
categories: ['Deep Learning from Scratch']
tag: ['deep learning', 'Backpropagation']
use_math: true
---

Numerical differentication is a simple way to calculate gradient descent(gradient descent of loss function for the parameters). 
However, it takes too long to compute.  
Instead, we can use back propagation which can compute gradient descent more efficiently. 
There are two ways to understand back propagation:  
1. By the equation(a general way in machine learning)  
2. By the computational graph  

## Computational Graph
We visualize the process of computing with node and edge(the line between nodes).
Forward propagation is the way to move from the input layer (left) to the output layer (right) in the neural network. 
The process of moving from the right to left i.e backward from the output to the input layer is called the back propagation. 
back propagation is important when calculating differentiation.  

### Advantages of Computational Graph
Since each node contains simple calculation, we can make the computing simpler no matter how complicated the calculation process is.
Also, computational graph is an effective way to calculate mutiple differentiation problems.

## Chain Rule
The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic programming. To understand what chain rule is, we need to talk about composite function first.  
In mathematics, function composition is an operation that takes two functions f and g and produces a function h such that h(x) = g.
For example, a fucntion  $z = (x+y)^2$ consists of  
$z = t^2$  
$t = x + y$  
In short, we can define function composition:
#### the differentiation of composite function can be expressed with mutiplication of functions that compose the composite function.

## Back Propagation
To understand back propagation, we will take simple addition/multiplication problems as examples.

### Back Propagation of Addition Nodes
The differentiation of $z = x + y$ is  
$\frac{\partial z}{\partial x} = 1$  
$\frac{\partial z}{\partial y} = 1$  
  
![img](https://user-images.githubusercontent.com/69702946/157623124-43f67d10-e40e-4863-bb7a-453335737324.png)  

```python
class AddLayer:
    def __init__(self):
        pass

    def forward(self, x, y):
        out = x + y

        return out

    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1

        return dx, dy
```


### Back Propagation of Multiplication Nodes
The differentiation of $z = xy$ is  
$\frac{\partial z}{\partial x} = y$  
$\frac{\partial z}{\partial y} = x$  
  
![img (1)](https://user-images.githubusercontent.com/69702946/157628254-07ea7b0a-f365-4313-958b-eb9ce57c7d5a.png)



```python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None

    def forward(self, x, y):
        self.x = x
        self.y = y                
        out = x * y

        return out

    def backward(self, dout):
        dx = dout * self.y
        dy = dout * self.x

        return dx, dy
```


```python

```
