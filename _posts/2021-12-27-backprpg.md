---
layout: single
title:  "Chapter5. Backpropagation"
categories: ['Deep Learning from Scratch']
tag: ['deep learning', 'Backpropagation']
---

Numerical differentication is a simple way to calculate gradient descent(gradient descent of loss function for the parameters). 
However, it takes too long to compute.  
Instead, we can use back propagation which can compute gradient descent more efficiently. 
There are two ways to understand back propagation:  
1. By the equation(a general way in machine learning)  
2. By the computational graph  

## Computational Graph
We visualize the process of computing with node and edge(the line between nodes).
Forward propagation is the way to move from the input layer (left) to the output layer (right) in the neural network. 
The process of moving from the right to left i.e backward from the output to the input layer is called the back propagation. 
back propagation is important when calculating differentiation.  

### Advantages of Computational Graph
Since each node contains simple calculation, we can make the computing simpler no matter how complicated the calculation process is.
Also, computational graph is an effective way to calculate mutiple differentiation problems.

## Chain Rule
Back propagation follows chain rule. To understand what chain rule is, we need to talk about composite function first.  
In mathematics, function composition is an operation that takes two functions f and g and produces a function h such that h(x) = g.
For example,  
$
