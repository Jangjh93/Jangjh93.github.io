---
layout: single
title:  "Chapter4. Neural Network Training"
categories: ['Deep Learning from Scratch']
tag: ['deep learning', 'neural network']
---

Neural network training is a process that will automatically produce the optimized weight values.
A loss function is function that shows error rate of prediction. An optimization problem seeks to minimize a loss function.
We would need to find weight and bias to minimize a loss function.


### Sum of Squares for Error(SSE)
E = 1/2 ∑(yk-tk)^2 where   
yk stands for the output value  
tk stands for the label  
k stands for the dimension  
 
```python
import numpy as np
```


```python
# Define Sum of Squres for Error(SSE)
def sum_squares_error (y,t):
    return 0.5 * np.sum((y-t)**2)
```


```python
# When the output predicted the probability
# of the correct answer '2' is the higest(0.6)
# -> predicted correctly
y = [.1, .05, .6, 0, .05, .1, 0, .1, 0, 0] # output values
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # label(the answer is actually '2')
```


```python
sum_squares_error(np.array(y), np.array(t))
```




    0.09750000000000003




```python
# When the output predicted the probability
# of the correct answer '7' is the higest(0.6)
# -> predicted incorrectly
y = [.1, .05, 0.1, 0, .05, .1, 0, .6, 0, 0] # output values
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # label(the answer is actually '2')
```


```python
sum_squares_error(np.array(y), np.array(t))
```




    0.5975

### Cross Entropy Error(CEE)
E = -∑tk * logyk where  
yk is the output  
tk is the label(only the index for the correct answer is 1 and 0 for else - One Hot Encoding).  

```python
# Define Cross Entropy Error(CEE)
def cross_entropy_error(y,t):
    delta = 1e-7 #preventing an overflow
    return -np.sum(t * np.log(y+delta))
```


```python
# When the output predicted the probability
# of the correct answer '2' is the higest(0.6)
# -> predicted correctly
y = [.1, .05, .6, 0, .05, .1, 0, .1, 0, 0] # output values
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # label(the answer is actually '2')
```


```python
cross_entropy_error(np.array(y),np.array(t))
```




    0.510825457099338




```python
# When the output predicted the probability
# of the correct answer '7' is the higest(0.6)
# -> predicted incorrectly
y = [.1, .05, 0.1, 0, .05, .1, 0, .6, 0, 0] # output values
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # label(the answer is actually '2')
```


```python
cross_entropy_error(np.array(y),np.array(t))
```




    2.302584092994546


### Mini Batch
When we have multiple training dataset, we need to use sum of loss functions of each training set.
In equation,  
E = -1/N ∑ ∑tnk logyk  

However, as the size of dataset gets bigger, computing gets costly. It's not realistict to compute loss function in big data.
This is why we use mini batch which randomly select a small number of data and train them.

```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test ) = \
    load_mnist(normalize = True, one_hot_label = True)

print(x_train.shape)
print(t_train.shape)
```

    (60000, 784)
    (60000, 10)
    


```python
train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```


```python
# When label is one hot encoded 
def cross_entropy_error(y,t):
    if y.ndim == 1:
        t = t.reshape(1, t.size) # label
        y = y.reshape(1, y.size) # output
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y+1e-7)) / batch_size
```


```python
# When label is NOT one hot encoded 
def cross_entropy_error(y,t):
    if y.ndim == 1:
        t = t.reshape(1, t.size) # label
        y = y.reshape(1, y.size) # output
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7))/batch_size
```

### Why Using Loss Function?
To answer this question, we need to look into how "differentiation"(inclination) works in neural network.  
In neural network training, to optimize the values of parameters(weight and bias), it minimizes loss function by calculation inclination of parameters.
For example, in a neural network when a differentiated parameter has a negative value, it can be optimized by moving it forward to a positive value.

Accuracy can't be an indicator how good the model is in neural network because accuracy, precision, and recall aren't differentiable, so we can't use them to optimize our machine learning models.

## Numerical Differentiation
Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.

### Differentiation
Differentiation is a process of finding a function that outputs the rate of change of one variable with respect to another variable.
In equation,  
![image2021_12_18](https://user-images.githubusercontent.com/69702946/146631306-e8df912d-44a1-47e3-b9b3-047d9a65bb5a.png)  
```python
def numerical_diff(f,x):
    h = 1e-4 # 0.0001
    return(f(x+h)) - f(x-h)/(2*h)
```
NOTE: This is called numerical differentiation. Numerical methods use exact algorithms to present numerical solutions to mathematical problems. Analytical is exact; numerical is approximate. For example, some differential equations cannot be solved exactly (analytic or closed form solution) and we must rely on numerical techniques to solve them. 


### An Example of Numerical Differentiation
```python
# When y = 0.01x^2 + 0.1x
def function_1(x):
    return 0.01*x**2 + 0.1*x
```


```python
import numpy as np
import matplotlib.pyplot as plt
x = np.arange(0.0, 20.0, 0.1)
y = function_1(x)
plt.xlabel("x")
plt.ylabel("f(x)")
plt.plot(x,y)
plt.show()
```


![output_2_0](https://user-images.githubusercontent.com/69702946/146631698-51e71551-35e5-401d-9d67-ef5eced9673d.png)




```python
# When x = 5
numerical_diff(function_1,5)
```




    0.1999999999990898




```python
# when x = 10
numerical_diff(function_1, 10)
```




    0.2999999999986347
    
    
### Partial Differential  
f(x0, x1) = x0^2 + x1^2

